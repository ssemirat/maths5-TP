{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP7 : Réseaux de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Na-Yck4lZiN1"
   },
   "source": [
    "## Chargement des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 530,
     "status": "ok",
     "timestamp": 1606831889555,
     "user": {
      "displayName": "Stephan Semirat",
      "photoUrl": "",
      "userId": "06413469689434144964"
     },
     "user_tz": -60
    },
    "id": "Zupdaax6Xf8b"
   },
   "outputs": [],
   "source": [
    "# Bibliothèques utilisées\n",
    "\n",
    "from matplotlib import pyplot as plt # graphique\n",
    "import numpy as np # mathématiques\n",
    "print('Bibliothèques chargées')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problème A\n",
    "\n",
    "On considère les 16 données $(X_j,Y_j)$, $j\\in\\{0,\\ldots,15\\}$, suivantes:\n",
    "\n",
    "- `[[0,0,0,0],[0,0,0,1],[0,0,1,0],[0,0,1,1],[0,1,0,0],[0,1,0,1],[0,1,1,0],[0,1,1,1],[1,0,0,0],[1,0,0,1],[1,0,1,0],[1,0,1,1],[1,1,0,0],[1,1,0,1],[1,1,1,0],[1,1,1,1]]` pour la liste des valeurs de $X_j$\n",
    "- `[0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0]` pour la liste des valeurs de $Y_j$\n",
    "\n",
    "Les données $(X_j,Y_j)$, $j\\in\\{0,\\ldots,15\\}$, représentent des 16 carrés composés de 4 cases \n",
    "<table><tr><td >x1</td><td>x2</td></tr><tr><td>x3</td><td>x4</td></tr></table>\n",
    "chaque case pouvant prendre la valeur 0 ou 1, ainsi que la valeur $Y_j=0$ ou $Y_j=1$ selon que le carré comporte exactement une diagonale de 1 (les deux autres cases étant à 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 1**\n",
    "\n",
    "Rappeler (vu en cours) pourquoi il n'est pas possible de modéliser les données par une fonction $s:X\\mapsto \\sigma(h(X))$, c'est à dire vérifiant ($s(X_j)>0.5$ si $Y_j=1$) et $s(X_j)<0.5$ si $Y_j=0$), avec $h(X)=b+w_1x_1+w_2x_2+w_3x_3+w_4x_4$.\n",
    "\n",
    "*Indication*. Obtenir une contradiction sur la valeur de $h(1,1,1,1)$ à partir des conditions sur les valeurs de $h(1,0,0,1)$, $h(0,1,1,0)$ et $h(0,0,0,0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modélisation par un réseau à 1 couche cachée de 2 neurones\n",
    "\n",
    "Dans la suite, on cherche à déterminer une fonction définie par l'expression $$R_{W,W_1,W_2}(x_1,x_2,x_3,x_4)=\\sigma\\bigg(h_W\\bigg(\\sigma\\big(h_{W_1}(x_1,x_2,x_3,x_4)\\big),\\sigma\\big(h_{W_2}(x_1,x_2,x_3,x_4)\\big)\\bigg)\\bigg)$$ avec\n",
    "$$W=(b,w_1,w_2), \\text{ et }h_W(s_1,s_2)=b+w_1s_1+w_2s_2$$\n",
    "$$W_1=(b_1,w_{11},w_{12},w_{13},w_{14})\\text{ et }h_{W_1}(x_1,x_2,x_3,x_4)=b_1+w_{11}x_1+w_{12}x_2+w_{13}x_3+w_{14}x_4,$$\n",
    "$$W_2=(b_2,w_{21},w_{22},w_{23},w_{24})\\text{ et }h_{W_2}(x_1,x_2,x_3,x_4)=b_2+w_{21}x_1+w_{22}x_2+w_{23}x_3+w_{24}x_4,$$\n",
    "permettant d'approximer au mieux $Y_j$ à travers $R(x_{j1},x_{j2},x_{j3},x_{j4})$, pour chaque donnée $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enregistrement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdata=[[0,0,0,0],[0,0,0,1],[0,0,1,0],[0,0,1,1],[0,1,0,0],[0,1,0,1],[0,1,1,0],[0,1,1,1],[1,0,0,0],[1,0,0,1],[1,0,1,0],[1,0,1,1],[1,1,0,0],[1,1,0,1],[1,1,1,0],[1,1,1,1]]\n",
    "Ydata=[0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0]\n",
    "print('Données enregistrées')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 2**\n",
    "\n",
    "Les 16 carrés sont enregistrés en Python sous forme de lignes de 4 valeurs, numérotées de 0 à 15.\n",
    "\n",
    "Quelles sont les deux lignes correspondant à un carré comportant exactement une diagonale de 1 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 3**\n",
    "\n",
    "Représenter (sur papier) par un graphe de calcul le réseau de neurones associé à la fonction $X\\mapsto R_{W,W1,W2}(X)$, où $X=(x_1,x_2,x_3,x_4)$, en faisant apparaitre, en colonnes, de gauche à droite:\n",
    "- en première colonne: les noeuds $1$, $x_1$, $x_2$, $x_3$, $x_4$;\n",
    "- en deuxième colonne: les noeuds $h_{W_1}(X)$, $h_{W_2}(X)$;\n",
    "- en troisième colonne: les noeuds $1$, $s_1=\\sigma(h_{W_1})$, $s_2=\\sigma(h_{W_2}(X))$\n",
    "- en quatrième colonne: le noeud $h_W(s_1,s_2)$\n",
    "- en cinquième colonne: le noeud $R_{W,W_1,W_2}=\\sigma(h_W)$\n",
    "\n",
    "puis d'une colonne à l'autre, les arrêtes pondérées par les paramètres impliqués dans les calculs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cellule ci-dessous définit la fonction `R(W,W1,W2,j)`, qui retourne la valeur de $R_{W,W_1,W_2}(X_j)$ avec comme paramètres entrant :\n",
    "- `W`, qui représente la liste `[b,w_1,w_2]` (paramètres associés à $h_W$)\n",
    "- `W1` et `W2`, qui représentent les listes `[b1,w11,w12,w13,w14]` (paramètres associés à $h_{W_1}$) et `[b2,w21,w22,w23,w24]` (paramètres associés à $h_{W_2}$) respectivement\n",
    "- `j`, qui représente le numéro $j$ de la donnée considérée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réseau\n",
    "def sigma(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def R(W,W1,W2,j):\n",
    "    Xj=Xdata[j] # Xj est la ligne j du tableau Xdata\n",
    "    Xj=np.array([[1],[Xj[0]],[Xj[1]],[Xj[2]],[Xj[3]]]) # mise sous forme de vecteur colonne, complété par 1 pour la multiplication par b\n",
    "    hW1=(np.array(W1) @ Xj)[0] # @ correspond au produit matricielle\n",
    "    hW2=(np.array(W2) @ Xj)[0]    \n",
    "    s1=sigma(hW1)\n",
    "    s2=sigma(hW2)\n",
    "    s=np.array([[1],[s1],[s2]])\n",
    "    hW=(np.array(W) @ s)[0]\n",
    "    R=sigma(hW)\n",
    "    return R\n",
    "\n",
    "print('Fonction sigma et R enregistrées')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 4**\n",
    "\n",
    "1. Vérifier à l'aide de la cellule suivante que les paramètres donnés sont tels que $R_{W,W_1,W_2}(X_j)>0.5$ si $Y_j=1$, et $R_{W,W_1,W_2}(X_j)<0.5$ si $Y_j=0$, comme souhaité.\n",
    "\n",
    "2. Donner l'expression complète de la fonction $R_{W,W_1,W_2}$ ainsi définie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=[-25,28,-13]\n",
    "W1=[0,-3,1.5,1.5,9]\n",
    "W2=[-14,-4,4,4,14]\n",
    "\n",
    "for j in range(16):\n",
    "    print('Y observé :',Ydata[j],' Y calculé par la fonction R:',R(W,W1,W2,j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage du réseau de neurones par descente de gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La détermination de paramètres `W=[b,w1,w2]`, `W1=[b1,w11,w12,w13,w14]` et `W2=[b2,w21,w22,w23,w24]` permettant d'approximer la relation entre $X$ et $Y$ par $Y=R_{W,W_1,W_2}(X)$ se fait, à partir des données, par minimisation de la fonction définie par $$f(W,W_1,W_2)=\\frac{1}{2}\\sum\\limits_{j=0}^{15}(R_{W,W_1,W_2}(X_j)-Y_j)^2.$$\n",
    "\n",
    "La minimisation se fait par descente de gradient.\n",
    "\n",
    "Puisque la dérivée d'une somme est la somme des dérivées, le gradient de $f$ est égal à la somme des gradients des fonctions $E_j$, définies pour chaque $j$ par $$E_j(W,W_1,W_2)=\\frac{1}{2}(R_{W,W_1,W_2}(X_j)-Y_j)^2.$$\n",
    "\n",
    "La fonction python `gradE` ci-dessous permet de calculer *par retropropagation* le gradient de $E_j$ par rapport à *chacun des paramètres* contenu dans les listes $W$, $W_1$ et $W_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient par rétropropagation\n",
    "\n",
    "def gradE(W,W1,W2,j):\n",
    "    b,w1,w2=W[0],W[1],W[2]\n",
    "    b1,w11,w12,w13,w14=W1[0],W1[1],W1[2],W1[3],W1[4]\n",
    "    b2,w21,w22,w23,w24=W2[0],W2[1],W2[2],W2[3],W2[4]\n",
    "    Xj=Xdata[j]\n",
    "    Xj1=Xj[0]\n",
    "    Xj2=Xj[1] \n",
    "    Xj3=Xj[2]\n",
    "    Xj4=Xj[3]\n",
    "    Yj=Ydata[j]\n",
    "    h1=b1+w11*Xj1+w12*Xj2+w13*Xj3+w14*Xj4\n",
    "    h2=b2+w21*Xj1+w22*Xj2+w23*Xj3+w24*Xj4    \n",
    "    H=b+w1*sigma(h1)+w2*sigma(h2)\n",
    "    sH=sigma(H)\n",
    "    sh1=sigma(h1)\n",
    "    sh2=sigma(h2)\n",
    "    dsH=sH*(1-sH)\n",
    "    dsh1=sh1*(1-sh1)\n",
    "    dsh2=sh2*(1-sh2)\n",
    "    dEdsH=sH-Yj\n",
    "    dsHdb=dsH*1\n",
    "    dsHdw1=dsH*sh1\n",
    "    dsHdw2=dsH*sh2\n",
    "    dsHdsh1=dsH*w1\n",
    "    dsHdsh2=dsH*w2\n",
    "    dsh1db1=dsh1*1\n",
    "    dsh1dw11=dsh1*Xj1\n",
    "    dsh1dw12=dsh1*Xj2\n",
    "    dsh1dw13=dsh1*Xj3\n",
    "    dsh1dw14=dsh1*Xj4\n",
    "    dsh2db2=dsh2*1\n",
    "    dsh2dw21=dsh2*Xj1\n",
    "    dsh2dw22=dsh2*Xj2\n",
    "    dsh2dw23=dsh2*Xj3\n",
    "    dsh2dw24=dsh2*Xj4\n",
    "    dEdb=dEdsH*dsHdb\n",
    "    dEdw1=dEdsH*dsHdw1\n",
    "    dEdw2=dEdsH*dsHdw2\n",
    "    dEdb1=dEdsH*dsHdsh1*dsh1db1\n",
    "    dEdw11=dEdsH*dsHdsh1*dsh1dw11\n",
    "    dEdw12=dEdsH*dsHdsh1*dsh1dw12\n",
    "    dEdw13=dEdsH*dsHdsh1*dsh1dw13\n",
    "    dEdw14=dEdsH*dsHdsh1*dsh1dw14\n",
    "    dEdb2=dEdsH*dsHdsh2*dsh2db2\n",
    "    dEdw21=dEdsH*dsHdsh2*dsh2dw21\n",
    "    dEdw22=dEdsH*dsHdsh2*dsh2dw22\n",
    "    dEdw23=dEdsH*dsHdsh2*dsh2dw23\n",
    "    dEdw24=dEdsH*dsHdsh2*dsh2dw24\n",
    "    return [[dEdb,dEdw1,dEdw2],[dEdb1,dEdw11,dEdw12,dEdw13,dEdw14],[dEdb2,dEdw21,dEdw22,dEdw23,dEdw24]]\n",
    "\n",
    "print('Fonction gradE enregistrée')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 5**\n",
    "1. Donner l'expression mathématique de la fonction $E_{10}$.\n",
    "2. Compléter le graphe de calcul de la fonction $R$ par le calcul de la fonction $E_{10}$.\n",
    "3. Rappeler l'expression simplifiée de dérivée de la composée $x\\mapsto\\sigma(u(x))$.\n",
    "4. Expliciter le calcul de $\\frac{\\partial E_{10}}{\\partial w_{1}}(W,W_1,W_2)$ et de $\\frac{\\partial E_{10}}{\\partial w_{23}}(W,W_1,W_2)$ par rétropropagation des dérivées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 6**\n",
    "\n",
    "La cellule ci-dessous permet de calculer $R_{W,W_1,W_2}(X_{10})$ et $\\frac{\\partial E_{10}}{\\partial w_{23}}(W,W_1,W_2)$ selon les paramètres :\n",
    "$$W=(b,w_1,w_2)=(0,2,-1)$$\n",
    "$$W_1=(b_1,w_{11},w_{12},w_{13},w_{14})=(0,-3,2,2,10)$$\n",
    "$$W_2=(b_1,w_{21},w_{22},w_{23},w_{24})=(-4,-1,1,1,3).$$\n",
    "\n",
    "Executer cette cellule pour répondre aux questions suivantes :\n",
    "1. Aux paramètres considérés, par rapport à quel(s) paramètre(s) la fonction $E_{10}$ est-elle croissante, décroissante, de dérivée nulle ?\n",
    "2. Dans quelle direction faut-il alors faire varier chacun des paramètres afin de diminuer $E_{10}$ ?\n",
    "3. En diminuant $E_{10}$, la fonction $f$ diminue-t-elle nécessairement ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=[0,2,-1]\n",
    "W1=[0,-3,2,2,10]\n",
    "W2=[-4,-1,1,1,3]\n",
    "j=10\n",
    "print('Valeur de E10:\\n',1/2*(R(W,W1,W2,j)-Ydata[j])**2)\n",
    "print('Valeur de R :\\n',R(W,W1,W2,j))\n",
    "print('Gradient de E10 :\\n',gradE(W,W1,W2,j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme de descente de gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction suivante implémente une *version partielle* de l'algorithme de descente de gradient.\n",
    "\n",
    "Comme il y a 16 données, la fonction $f$ à minimiser est une somme de 16 fonctions $E_j$, chacune dépendant des paramètres.\n",
    "\n",
    "L'algorithme de descente gradient, visant à minimiser $f$, doit rendre chacune des $E_j$ petite, *sans augmenter les autres*.\n",
    "\n",
    "La version de l'algorithme de descente de gradient ci-dessous effectue une descente de gradient sur seulement 5 termes de la somme $f$ des $E_j$; ces termes sont *choisis au hasard* à chaque pas de l'algorithme.\n",
    "\n",
    "Cette méthode (standard) diminue les calculs effectués à chaque pas de l'algorithme, tout en permettant réduire la valeur de $f$ de manière probabiliste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descente(W,W1,W2,tau=0.01,tolerance=1e-2,NbIterationsMax=1000):\n",
    "    for i in range(NbIterationsMax):\n",
    "        batch=np.random.randint(0, 16, 5) # à chaque itération, 5 entiers aléatoires sont choisis entre 0 et 15\n",
    "        g=[[0,0,0],[0,0,0,0,0],[0,0,0,0,0]] # gradient temporairement initialisé à un gradient nul\n",
    "        for j in batch: # j parcourt 5 données aléatoires\n",
    "            gj=gradE(W,W1,W2,j) # calcul du gradient gg de la fonction Ej\n",
    "            g = [np.add(g[0],gj[0]),np.add(g[1],gj[1]),np.add(g[2],gj[2])] # ajout du gradient gj au gradient g\n",
    "        try: # traitement des erreurs si l'algorithme diverge\n",
    "            if (W[0]==float('inf')) or (W[1]==float(\"inf\")) or (W[2]==float(\"inf\"))\\\n",
    "            or (W1[0]==float('inf')) or (W1[1]==float(\"inf\")) or (W1[2]==float(\"inf\")) or (W1[3]==float(\"inf\")) or (W1[4]==float(\"inf\"))\\\n",
    "            or (W2[0]==float('inf')) or (W2[1]==float(\"inf\")) or (W2[2]==float(\"inf\")) or (W2[3]==float(\"inf\")) or (W2[4]==float(\"inf\")):\n",
    "                raise(OverflowError)\n",
    "            ng=np.sqrt(np.sum([np.sum([gi**2 for gi in g[k]]) for k in range(2)]))/len(batch) # somme des carrés des coordonnées de g\n",
    "            if ng<tolerance:\n",
    "                print('L\\'algorithme a convergé en',i,'itérations. \\nSolution atteinte :\\n W=',W,'\\n W1=',W1,'\\n W2=',W2,'\\nGradient :',g,'\\n Norme :',ng)\n",
    "                return [W,W1,W2]\n",
    "            W=[W[k]-tau*g[0][k] for k in range(3)]\n",
    "            W1=[W1[k]-tau*g[1][k] for k in range(5)]\n",
    "            W2=[W2[k]-tau*g[2][k] for k in range(5)]\n",
    "        except OverflowError as err: # traitement de l'erreur \"overflow\"\n",
    "            print('L\\'algorithme a divergé \\nSolution atteinte :\\n W=',W,'\\n W1=',W1,'\\n W2=',W2,'\\nGradient :',g,'\\n Norme :',ng)\n",
    "            return [W,W1,W2]\n",
    "    print('L\\'algorithme n\\'a pas convergé \\nSolution atteinte :\\n W=',W,'\\n W1=',W1,'\\n W2=',W2,'\\nGradient :',g,'\\n Norme :',ng)\n",
    "    return [W,W1,W2]\n",
    "\n",
    "print('Fonction descente enregistrée')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cellule suivante initialise l'algorithme à des valeurs abitraire et,  avec une tolérance de $10^{-3}$, devrait faire converger l'algorithme.\n",
    "\n",
    "Les paramètres obtenus sont enregistrés dans la variable `parametres` renvoyée par l'algorithme, sous la forme d'une liste `[W,W1,W2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# la variable parametres retournée est égale à [W,W1,W2], atteint par l'algorithme\n",
    "\n",
    "parametres=descente(W=[1,0,0],W1=[1,1,1,1,1],W2=[1,0,0,0,0],tau=0.1,tolerance=0.001,NbIterationsMax=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice**\n",
    "\n",
    "La cellule suivante récupère les paramètres obtenus par la descente de gradient, afin de vérifier que la fonction $R$ ainsi obtenue sépare bien les données.\n",
    "\n",
    "Effectuer cette vérification en exécutant la cellule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=parametres[0]\n",
    "W1=parametres[1]\n",
    "W2=parametres[2]\n",
    "for j in range(16):\n",
    "    print('Y observé :',Ydata[j],' Y calculé par R :',R(W,W1,W2,j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problème B\n",
    "\n",
    "Les cellules suivantes permettent de déterminer les coefficients d'un réseau de neurones qui identifie si un carré de 4 cases contient exactement une ligne de 1.\n",
    "\n",
    "À cette fin, vous devrez au préalable modifier les données `Ydata`, en modifiant les valeurs correspondant aux carrés visés :\n",
    "- Ydata[j] doit être égal à 1 si le carré contient exactement une ligne de 1\n",
    "- Ydata[j] doit être égal à 0 si le carré ne contient pas exactement une ligne de 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description des carrés\n",
    "Xdata=[[0,0,0,0],[0,0,0,1],[0,0,1,0],[0,0,1,1],[0,1,0,0],[0,1,0,1],[0,1,1,0],[0,1,1,1],[1,0,0,0],[1,0,0,1],[1,0,1,0],[1,0,1,1],[1,1,0,0],[1,1,0,1],[1,1,1,0],[1,1,1,1]]\n",
    "# Description des Y correspondants : A MODIFIER\n",
    "Ydata=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "print('Données enregistrées')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des paramètres par descente de gradient pour ces nouvelles données :\n",
    "\n",
    "parametres=descente(W=[1,0,0],W1=[1,1,1,1,1],W2=[1,0,0,0,0],tau=0.1,tolerance=0.001,NbIterationsMax=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification des valeurs de la fonction R obtenue :\n",
    "\n",
    "W=parametres[0]\n",
    "W1=parametres[1]\n",
    "W2=parametres[2]\n",
    "for j in range(16):\n",
    "    print('Y observé :',Ydata[j],' Y calculé par R :',R(W,W1,W2,j))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOL5iJTlZCmM9764vTsZljv",
   "collapsed_sections": [],
   "name": "descente-deux-variables.ipynb",
   "provenance": [
    {
     "file_id": "164i4yxW6Yks9BXBhzNAJHUMyX1UBjEp3",
     "timestamp": 1606828072802
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
